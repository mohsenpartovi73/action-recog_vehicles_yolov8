# -*- coding: utf-8 -*-
"""YOLOv8.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/mohsenpartovi73/YOLOv8_vehicles_object_detection/blob/main/YOLOv8.ipynb
"""

import os
HOME = os.getcwd()
print(HOME)

# Pip install method (recommended)

!pip install ultralytics==8.2.103 -q

from IPython import display
display.clear_output()

import ultralytics
ultralytics.checks()

from ultralytics import YOLO

from IPython.display import display, Image

# Commented out IPython magic to ensure Python compatibility.
!mkdir -p {HOME}/datasets
# %cd {HOME}/datasets

!pip install roboflow
from roboflow import Roboflow
rf = Roboflow(api_key="Kerw8HPgNNQSSCGteOGa")
project = rf.workspace("roboflow-gw7yv").project("vehicles-openimages")
version = project.version(1)
dataset = version.download("yolov8")

# Commented out IPython magic to ensure Python compatibility.
# %cd {HOME}

!yolo task=detect mode=train model=yolov8s.pt data={dataset.location}/data.yaml epochs=25 imgsz=800 plots=True

# Commented out IPython magic to ensure Python compatibility.
# %cd {HOME}
!yolo task=detect mode=predict model='/content/yolov8s.pt' conf=0.25 source='/content/runs/IMG_0001.JPG' save=True

Image(filename='/content/runs/detect/predict2/IMG_0001.JPG', height=600)

!Python 3.5.6 |Anaconda custom (64-bit)| (default, Aug 26 2018, 16:05:27) [MSC v.1900 64 bit (AMD64)]

import cv2
from ultralytics import YOLO
from google.colab import files

# Upload your video file
uploaded = files.upload()

# Load the YOLOv8 model (make sure you have the correct path)
model = YOLO('/content/yolov8s.pt')  # Use a valid YOLOv8 model path

# Get the name of the uploaded video file
video_file = next(iter(uploaded))

# Open the video file
cap = cv2.VideoCapture(video_file)

# Check if video opened successfully
if not cap.isOpened():
    print("Error: Could not open video.")
    exit()

# Read the first frame to check if it is working
ret, frame = cap.read()
if ret:
    cv2_imshow(frame)  # Display the first frame
else:
    print("Error: Could not read frame.")
    exit()

# Reset video capture to start processing from the beginning
cap.set(cv2.CAP_PROP_POS_FRAMES, 0)

while cap.isOpened():
    ret, frame = cap.read()
    if not ret:
        break

    # Perform inference on the frame
    results = model(frame)

    # Check results
    if results:
        for result in results:
            boxes = result.boxes  # Get boxes
            for box in boxes:
                x1, y1, x2, y2 = map(int, box.xyxy[0])  # Get bounding box coordinates
                conf = box.conf[0]  # Get confidence score
                cls = box.cls[0]  # Get class id

                # Only display detections with confidence above a threshold (e.g., 0.5)
                if conf > 0.5:
                    label = f'Class {int(cls)} {conf:.2f}'  # Customize label based on your classes
                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
                    cv2.putText(frame, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)

    # Display the resulting frame using matplotlib
    from google.colab.patches import cv2_imshow
    cv2_imshow(frame)

# Release the capture object
cap.release()

